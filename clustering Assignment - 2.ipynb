{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c961edb0-ca7e-4a96-8687-bc018ae79a69",
   "metadata": {},
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f72373-e67d-4ca6-89a8-d7cc073ee7a5",
   "metadata": {},
   "source": [
    "Hierarchical clustering is a clustering technique that builds a tree-like hierarchy of clusters. Unlike partitioning algorithms like K-means, hierarchical clustering does not require specifying the number of clusters beforehand and produces a nested sequence of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e725e2-bb90-40b5-a5b0-eec915a0dc23",
   "metadata": {},
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f3f3d-7b28-4578-ab3f-2069e2651480",
   "metadata": {},
   "source": [
    "# The two main types of hierarchical clustering algorithms are:\n",
    "\n",
    "Agglomerative hierarchical clustering: It starts with each data point as a separate cluster and merges the closest pairs of clusters iteratively until all points belong to a single cluster.\n",
    "Divisive hierarchical clustering: It starts with all data points in one cluster and splits the clusters recursively into smaller clusters until each cluster contains only one data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea2f5a-6c59-474f-962b-3350c0faa3c0",
   "metadata": {},
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the\n",
    "common distance metrics used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f0964-282c-4aa7-8196-b8bde3f3ed83",
   "metadata": {},
   "source": [
    "The distance between two clusters in hierarchical clustering is determined using a distance metric, such as Euclidean distance, Manhattan distance, or cosine similarity. Common distance metrics include:\n",
    "\n",
    "Euclidean distance\n",
    "\n",
    "Manhattan distance\n",
    "\n",
    "Cosine similarity\n",
    "\n",
    "Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b7a8c-db67-4832-954d-81920502eab4",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some\n",
    "common methods used for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551babc0-c9ac-4345-a0d9-9cb686dff0f3",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters in hierarchical clustering can be challenging as it does not produce a single partition. Common methods for determining the number of clusters include examining the dendrogram for natural cut points, using the elbow method, and assessing clustering stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987926a3-0c2a-47bd-93d3-75555bc134a0",
   "metadata": {},
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c6546-9b0b-40a3-acfa-eb33cfa0beed",
   "metadata": {},
   "source": [
    "Dendrograms in hierarchical clustering are tree-like structures that illustrate the arrangement of clusters and their merging process. They are useful for visualizing the hierarchical relationships between clusters and identifying natural groupings or clusters at different levels of granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca05c7-9035-445c-a402-f24521cc3a11",
   "metadata": {},
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the\n",
    "distance metrics different for each type of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e98762-001b-4546-ad11-20459f70b973",
   "metadata": {},
   "source": [
    "Yes, hierarchical clustering can be used for both numerical and categorical data. However, the distance metrics used for each type of data may differ. For numerical data, distance metrics like Euclidean distance or Manhattan distance are commonly used, while for categorical data, metrics like the Jaccard distance or Gower distance may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf196e36-a604-42b9-90a9-5d7f5bfff60e",
   "metadata": {},
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a134ec-473e-4c4c-8881-f4bd98121058",
   "metadata": {},
   "source": [
    "Hierarchical clustering can be used to identify outliers or anomalies by examining the distance between data points and clusters. Outliers are often located far from any cluster or have high dissimilarity to other data points. By setting a threshold distance, data points that are significantly distant from their nearest clusters can be identified as outliers. Additionally, clustering algorithms can be combined with outlier detection techniques to improve anomaly detection in hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424543a5-68bf-4bd3-b1a7-c4e9d334a2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
